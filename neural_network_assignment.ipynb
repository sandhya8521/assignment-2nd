{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) What is deep learning, and how is it connected to artificial intelligence\" ?\n",
        "\n",
        "ANS:\n",
        "\n",
        "*   Deep learning is a subfield of machine learning, which itself is a branch of artificial intelligence (AI).\n",
        "*   It focuses on algorithms inspired by the structure and function of the human brain, known as artificial neural networks\n",
        "\n",
        "*   These algorithms are designed to learn from large amounts of data by identifying patterns, making decisions, and performing tasks without being explicitly programmed for specific rules.\n",
        "\n",
        "How is Deep Learning Connected to Artificial Intelligence?\n",
        "\n",
        "ans\n",
        "\n",
        "1.  AI Umbrella:\n",
        "\n",
        " Artificial Intelligence is the broader concept of creating systems capable of intelligent behavior. It includes various subfields like machine learning, robotics, computer vision, and natural language processing.\n",
        "\n",
        "2.  Machine Learning as a Foundation:\n",
        "\n",
        " Machine Learning (ML) is a subset of AI focused on algorithms that allow computers to learn from data. Deep learning is a specific approach within ML.\n",
        "\n",
        "1.  Deep Learning's Role in AI:\n",
        "\n",
        "*   Deep learning enables AI systems to achieve advanced capabilities such as:\n",
        " *   Understanding speech (e.g., voice assistants like Siri and Alexa)\n",
        "\n",
        " *  Recognizing images and objects (e.g., facial recognition systems)\n",
        "\n",
        " *   Driving autonomous cars (e.g., Tesla's autopilot system)\n",
        " *   Generating creative content (e.g., AI art and music)\n",
        "\n",
        "4.   State-of-the-Art Results:\n",
        " Deep learning is often the preferred approach for cutting-edge AI applications due to its high accuracy and adaptability in complex environments\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-NmL1wA7pSO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.) What is a neural network, and what are the different types of neural networks!\n",
        "\n",
        "ANS\n",
        "\n",
        "A neural network is a computational model inspired by the structure and function of the human brain. It consists of interconnected layers of nodes (also called neurons) that process data in a way similar to biological neurons.\n",
        "\n",
        "Types of Neural Networks\n",
        "\n",
        "1. Feedforward Neural Networks (FNN)\n",
        "\n",
        " Description: The simplest type of neural network where connections between nodes do not form a cycle. Data flows in one direction—from input to output.\n",
        "\n",
        " Applications: Basic classification tasks, regression problems.\n",
        "\n",
        " Example: Predicting house prices, spam email detection.\n",
        "\n",
        "2. Convolutional Neural Networks (CNN)\n",
        "\n",
        " Description: Designed for processing grid-like data structures, such as images. It uses convolutional layers to detect spatial features like edges, shapes, and textures.\n",
        "\n",
        " Applications: Image classification, object detection, video analysis.\n",
        "\n",
        " Example: ImageNet classification, facial recognition.\n",
        "3. Recurrent Neural Networks (RNN)\n",
        "\n",
        " Description: Suitable for sequential data because of its \"memory\" capability, where outputs depend on prior inputs. It uses feedback loops to handle sequences.\n",
        "\n",
        " Applications: Time series forecasting, natural language processing (NLP), speech recognition.\n",
        "\n",
        " Example: Sentiment analysis, language translation.\n",
        "4. Long Short-Term Memory Networks (LSTM)\n",
        "\n",
        " Description: A type of RNN specifically designed to capture long-term dependencies in sequences. It mitigates the vanishing gradient problem of traditional RNNs.\n",
        "\n",
        " Applications: Text generation, stock price prediction.\n",
        "\n",
        " Example: ChatGPT-like conversational models.\n",
        "5.Generative Adversarial Networks (GANs)\n",
        "\n",
        " Description: Consists of two networks—a generator and a discriminator—competing against each other to generate realistic data.\n",
        "\n",
        " Applications: Image generation, style transfer, data augmentation.\n",
        "\n",
        " Example: Creating deepfake videos, generating synthetic data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CHxW4duvaQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)What is the mathematical structure of a neural network!\n",
        "\n",
        "ans\n",
        "\n",
        "The mathematical structure of a neural network involves a sequence of computations designed to transform input data into meaningful output. These computations are governed by linear algebra, calculus, and optimization principles.\n",
        "\n"
      ],
      "metadata": {
        "id": "jTK3DU639yrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.)What is an activation function, and why is it essential in neural\"\n",
        "\n",
        "ans\n",
        "\n",
        "An activation function is a mathematical function applied to a neuron's output (after the weighted sum and bias addition) in a neural network. It determines whether the neuron should be activated (output a signal) or not, based on its input. Activation functions introduce non-linearity into the network, enabling it to learn and model complex relationships.\n",
        "\n",
        " Why is an Activation Function Essential in Neural Networks?\n",
        "\n",
        "1. Introducing Non-Linearity:\n",
        "\n",
        "  Without an activation function, the neural network would behave like a linear regression model, regardless of the number of layers.\n",
        "\n",
        " Non-linear activation functions allow the network to learn complex patterns, relationships, and decision boundaries.\n",
        "2.Making Deep Networks Effective:\n",
        "\n",
        " With multiple layers, activation functions enable stacking of non-linear transformations, allowing deep networks to approximate any arbitrary function (Universal Approximation Theorem).\n",
        "\n",
        "3. Handling Complex Data:\n",
        "\n",
        " Real-world data (e.g., images, speech, and text) often involve intricate patterns. Activation functions provide the flexibility needed to model such data.\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "GdSeCwfb_E95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.)Could you list some common activation functions used in neural networks!\n",
        "\n",
        "ans\n",
        "\n",
        "Common Activation Functions in Neural Networks\n",
        "\n",
        "1. Linear Activation\n",
        ""
      ],
      "metadata": {
        "id": "XKTb_SkNAmFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.) What is a multilayer neural network!\n",
        "\n",
        "ans\n",
        "\n",
        "A multilayer neural network is a type of artificial neural network (ANN) that consists of multiple layers of neurons. These layers enable the network to model complex relationships and solve intricate problems through a hierarchy of transformations."
      ],
      "metadata": {
        "id": "aG-WN60ZQmN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.)What is a loss function, and why is it crucial for neural network training!\n",
        "\n",
        "ans\n",
        "\n",
        "A loss function is a mathematical function that quantifies the difference between a neural network's predicted output (\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " ) and the actual target value (\n",
        "y\n",
        "y). It serves as a measure of the model's performance during training, indicating how far off the predictions are from the ground truth"
      ],
      "metadata": {
        "id": "JhxgERVzQyd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " What are some common types of loss functions!\n",
        "\n",
        " ans\n",
        "\n",
        " Common Types of Loss Functions:\n",
        "\n",
        "  Loss functions are chosen based on the type of machine learning task (e.g., regression, classification). Below is an overview of widely used loss functions:\n",
        "\n",
        "1.For Regression Problems\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        " Description: Penalizes larger errors more than smaller ones by squaring the differences.\n",
        "\n",
        "Use Case: Predicting continuous values (e.g., house prices).\n",
        "\n",
        "Pros: Smooth gradient; easy optimization.\n",
        "\n",
        "Cons: Sensitive to outliers.\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Description: Measures the average magnitude of errors.\n",
        "\n",
        "Use Case: Robust against outliers.\n",
        "\n",
        "Pros: Less sensitive to large errors than MSE.\n",
        "\n",
        "Cons: Non-differentiable at zero.\n"
      ],
      "metadata": {
        "id": "bjlSHENnRTDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) How does a neural network learn!\n",
        "\n",
        "ans\n",
        "\n",
        "A neural network learns by adjusting its weights and biases to minimize the error (or loss) between its predicted outputs (\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " ) and the actual outputs (\n",
        "y\n",
        "y). This process is driven by forward propagation, loss calculation,  backpropagation, and optimization.\n",
        "\n",
        "\n",
        "* step-by-Step Learning Process\n",
        "\n",
        "`1. Initialization\n",
        "\n",
        "*   The network starts with random values for its weights and biases.\n",
        "*  These parameters will be updated iteratively during the training process\n",
        "\n",
        "2. Forward Propagation\n",
        "*  Objective: Pass input data through the network to make predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v7JH-CJ0SxWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)What is an optimizer in neural networks, and why is it necessary!\n",
        "\n",
        "ANS\n",
        "\n",
        "An optimizer is an algorithm or method used in neural networks to adjust the model's weights and biases iteratively during training to minimize the loss function. It determines how the parameters are updated based on the gradients computed during backpropagation.\n",
        "\n",
        "Why is an Optimizer Necessary?\n",
        "\n",
        "Optimizers are crucial for the training process because they:\n",
        "\n",
        "\n",
        "1.Facilitate Learning:\n",
        "\n",
        "*   sure the neural network converges to a solution by minimizing the loss function.\n",
        "\n",
        "2.Handle High-dimensional Problems:\n",
        "\n",
        "*   Manage the complex optimization of millions of parameters in large networks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1STvdpiIUm3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)Could you briefly describe some common optimizers!\n",
        "\n",
        "ANS\n",
        "\n",
        "1.Gradient Descent:\n",
        "\n",
        "Description: Updates parameters by moving in the direction of the negative gradient of the loss.\n",
        "\n",
        "*   Variants:\n",
        " *   Batch Gradient Descent: Uses the entire dataset for updates.\n",
        " *   Stochastic Gradient Descent (SGD): Updates using one data point at a time.\n",
        " *   Mini-Batch Gradient Descent: Combines advantages of both by using small subsets of data.\n",
        "\n",
        "2. RMSprop (Root Mean Square Propagation)\n",
        "*   Description: Scales learning rate based on the magnitude of recent gradients, helping in cases with non-stationary data.\n",
        "\n",
        "*   Use Case: Effective for recurrent neural networks and noisy objectives.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiATXvxkZZlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)Can you explain forward and backward propagation in a neural network!\n",
        "\n",
        "ANS\n",
        "\n",
        "Forward and Backward Propagation in a Neural Network\n",
        " Forward and backward propagation are the key processes that enable a neural network to learn from data.\n",
        "\n",
        "1.Forward Propagation\n",
        "\n",
        "Purpose: Compute the predicted output (\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " ) given an input, using the current weights and biases.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1.Input to the Network:\n",
        "*  Input features (\n",
        "x\n",
        "x) are passed to the network's input layer.\n",
        "\n",
        "2.Backward Propagation\n",
        "\n",
        "Purpose: Adjust weights and biases by minimizing the error between predicted (\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " ) and actual (\n",
        "y\n",
        "y) outputs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zRD-P79lbJzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.)What is weight initialization, and how does it impact training!\n",
        "\n",
        "ans\n",
        "\n",
        " Weight initialization is the process of assigning initial values to the weights of a neural network before training begins. The choice of initial weights can significantly impact the training process, including the speed of convergence and the likelihood of reaching a good solution.\n",
        "* Why is Weight Initialization Important?\n",
        "\n",
        "1. Avoiding Vanishing/Exploding Gradients:\n",
        "\n",
        "   *  Poor initialization can cause gradients to shrink (vanish) or grow (explode) during backpropagation, making it difficult to train the network.\n",
        "\n",
        "2. Ensuring Symmetry Breaking:\n",
        "  *  If all weights are initialized to the same value, all neurons in the same layer will produce identical outputs, preventing the network from learning effectively\n",
        "3.Improving Convergence Speed:\n",
        "  * Proper initialization helps the network converge faster by starting with reasonable gradients and loss values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "POes-VwQcoj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) What is the vanishing gradient problem in deep learning!\n",
        "\n",
        "ANS\n",
        "\n",
        "The vanishing gradient problem is a common issue in deep learning, especially when training deep neural networks with many layers. It occurs when gradients of the loss function (used for updating weights during backpropagation) become very small as they propagate backward through the network layers. This can lead to several problems:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C1quGuNvqRrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.)What is the exploding gradient problem?\n",
        "\n",
        "Ans\n",
        "\n",
        "The exploding gradient problem is another issue in deep learning, often observed during the training of deep neural networks. It occurs when gradients grow excessively large during backpropagation, leading to unstable weight updates. This can make the model's training process erratic or cause it to fail entirely.\n",
        "\n"
      ],
      "metadata": {
        "id": "9LHLRftjeSlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.)How do you create a simple perceptron for basic binary classification!\n",
        "\n",
        "ANS\n",
        "\n",
        "Creating a simple perceptron for binary classification involves building a basic neural network with one layer. A perceptron is a linear classifier that uses a step or activation function to predict the class label based on input features. Below is a step-by-step implementation in Python using NumPy.\n",
        "\n",
        "*  Steps to Create a Simple Perceptron\n",
        "  *  Import Libraries: Import necessary libraries like NumPy.\n",
        "  *  Initialize Parameters: Set up weights and bias.\n",
        "  *  Define the Activation Function: Use a step function for binary classification.\n",
        "  *  Train the Perceptron: Update weights and bias using gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3qKqcQpTfOkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)How can you build a neural network with one hidden layer using Keras!\n",
        "\n",
        "ans\n",
        "\n",
        "Building a neural network with one hidden layer using Keras is straightforward. Keras, a high-level neural networks library, provides an easy-to-use API for building and training deep learning models. Below is a step-by-step guide to building a simple feedforward neural network for binary classification.\n",
        "*    Steps to Build a Neural Network with One Hidden Layer:\n",
        "   *   Import Required Libraries: Use TensorFlow/Keras for model building and NumPy for data handling.\n",
        "   *  Prepare the Data: Create or load a dataset and split it into training and testing sets.\n",
        "   *   Define the Model Architecture: Specify the input layer, hidden layer, and output layer.\n",
        "   *  Compile the Model: Define the loss function, optimizer, and metrics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x5q7t0hzjIcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.)How do you initialize weights using the Xavier (Glorot) initialization method in Keras!\n",
        "\n",
        "ANS\n",
        "\n",
        "The Xavier (Glorot) initialization method is used to initialize the weights of neural networks to ensure a smooth flow of gradients during training. This method helps prevent the vanishing or exploding gradient problem by setting the initial weights in a range that maintains the variance of activations and gradients across layers.\n",
        "\n",
        "In Keras, you can use the GlorotUniform or GlorotNormal initializers to implement Xavier initialization. Here's how to do it:\n",
        "\n",
        "*   Steps to Use Xavier Initialization in Keras\n",
        "  1.   Import Xavier Initializer:\n",
        "      * Keras provides GlorotUniform (default in most cases) and GlorotNormal initializers.\n",
        "  2.  Define Layers with Xavier Initialization:\n",
        "     *   Specify the weight initializer for each layer using the kernel_initializer argument.\n",
        "  3.   Build the Model:\n",
        "     *  Add layers to your model with the Xavier initializer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aRDVNAMHk8s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation Example\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal\n",
        "\n",
        "# Step 1: Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Step 2: Add a hidden layer with Xavier initialization (GlorotUniform)\n",
        "model.add(Dense(8, input_dim=4, activation='relu', kernel_initializer=GlorotUniform()))\n",
        "\n",
        "# Step 3: Add an output layer with Xavier initialization (GlorotNormal)\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer=GlorotNormal()))\n",
        "\n",
        "# Step 4: Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "_xnOMUlNmfBg",
        "outputId": "c0e26ead-1712-49ea-8d19-7b2129f94c1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │              \u001b[38;5;34m40\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m9\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49\u001b[0m (196.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> (196.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49\u001b[0m (196.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> (196.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.)How can you apply different activation functions in a neural network in Keras!\n",
        "\n",
        "ANS\n",
        "\n",
        "Applying different activation functions in a neural network in Keras is straightforward. Each layer in Keras can be assigned an activation function using the activation argument. Keras supports various activation functions like ReLU, sigmoid, tanh, softmax, etc., which can be applied based on the layer's role in the network.\n",
        "\n",
        "Steps to Apply Different Activation Functions\n",
        "1.  Choose Activation Functions:\n",
        "\n",
        "*   Select activation functions based on the layer's purpose:\n",
        "\n",
        "  *  Hidden Layers: Use non-linear functions like ReLU, tanh, or Leaky ReLU.\n",
        "\n",
        "  *   Output Layer:\n",
        "\n",
        "      *   Sigmoid: Binary classification.\n",
        "      *   Softmax: Multi-class classification.\n",
        "      *   Linear: Regression tasks.\n",
        "2.  Assign Activation Functions:\n",
        "*   Specify the activation function using the activation parameter in the Dense (or other layer) constructor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QwzQneCGmqmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation Example\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, LeakyReLU\n",
        "\n",
        "# Step 1: Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer with ReLU activation\n",
        "model.add(Dense(64, input_dim=20))  # 20 input features\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Hidden Layer with Tanh activation\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('tanh'))\n",
        "\n",
        "# Hidden Layer with Leaky ReLU activation\n",
        "model.add(Dense(16))\n",
        "model.add(LeakyReLU(alpha=0.1))  # alpha = slope for negative values\n",
        "\n",
        "# Output Layer with Sigmoid activation for binary classification\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Step 2: Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "mOpHasken3Bx",
        "outputId": "475585e1-6c20-4023-b94a-bb71bb1b13e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m1,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,969\u001b[0m (15.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,969</span> (15.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,969\u001b[0m (15.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,969</span> (15.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6.)How do you manually implement forward propagation in a simple neural network!\n",
        "\n",
        "ANS\n",
        "\n",
        "Manually implementing forward propagation in a simple neural network involves performing the following steps:\n",
        "\n",
        "1. Initialize Parameters: Set up weights and biases for each layer.\n",
        "2. Activation Functions: Define activation functions to apply non-linearity.\n",
        "3. Compute Outputs Layer by Layer: Multiply inputs with weights, add biases, and apply the activation function for each layer.\n",
        "\n",
        " Let’s demonstrate forward propagation for a 3-layer neural network:\n",
        "\n",
        "Network Structure:-\n",
        " 1. Input Layer: 3 input features.\n",
        " 2.Hidden Layer: 2 neurons with ReLU activation.\n",
        " 3.Output Layer: 1 neuron with sigmoid activation for binary classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "zNVvtIXcn9YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step-by-Step Implementation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Initialize weights and biases\n",
        "# Input Layer -> Hidden Layer\n",
        "W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Shape: (3, 2)\n",
        "b1 = np.array([0.1, 0.2])  # Shape: (2,)\n",
        "\n",
        "# Hidden Layer -> Output Layer\n",
        "W2 = np.array([[0.7], [0.8]])  # Shape: (2, 1)\n",
        "b2 = np.array([0.3])  # Shape: (1,)\n",
        "\n",
        "# Input data\n",
        "X = np.array([[1.0, 2.0, 3.0]])  # Single input sample, Shape: (1, 3)\n",
        "\n",
        "# Step 2: Define activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Step 3: Forward propagation\n",
        "# Layer 1 (Input -> Hidden)\n",
        "Z1 = np.dot(X, W1) + b1  # Linear combination, Shape: (1, 2)\n",
        "A1 = relu(Z1)            # Activation, Shape: (1, 2)\n",
        "\n",
        "# Layer 2 (Hidden -> Output)\n",
        "Z2 = np.dot(A1, W2) + b2  # Linear combination, Shape: (1, 1)\n",
        "A2 = sigmoid(Z2)          # Activation (Output), Shape: (1, 1)\n",
        "\n",
        "# Final output\n",
        "print(\"Output of the network (A2):\", A2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exe8X2zBphQH",
        "outputId": "2fb49f82-59f8-4388-951c-4b6a63d9dc12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network (A2): [[0.98674452]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.)How do you add batch normalization to a neural network model in Keras!\n",
        "\n",
        "ANS\n",
        "\n",
        "Batch normalization (BatchNorm) is a technique used to normalize the inputs to each layer within a neural network. It improves training speed, stabilizes the learning process, and can reduce the sensitivity to initialization.\n",
        "\n",
        "In Keras, you can add batch normalization layers using the BatchNormalization class.\n",
        "\n",
        "Steps to Add Batch Normalization\n",
        "1. Import BatchNormalization:\n",
        "  *   Use from tensorflow.keras.layers import BatchNormalization.\n",
        "2. Insert BatchNormalization Layers:\n",
        "  *   Place BatchNormalization() between the linear transformation (Dense or convolution) and activation function.\n",
        "3.Model Compilation and Training:\n",
        "   *   Compile and train the model as usual.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IIDu7Lx7pu2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation Example: Adding Batch Normalization\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 1: Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(64, input_dim=20))\n",
        "model.add(BatchNormalization())  # Add batch normalization after the Dense layer\n",
        "model.add(Activation('relu'))    # Apply activation function\n",
        "\n",
        "# Hidden Layer\n",
        "model.add(Dense(32))\n",
        "model.add(BatchNormalization())  # Batch normalization for the hidden layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))  # Binary classification\n",
        "\n",
        "# Step 2: Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "7fObj4SIqUA2",
        "outputId": "7fac319c-f8b7-4f33-c4b8-f0dd5b54ebbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m1,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,841\u001b[0m (15.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,841</span> (15.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,649\u001b[0m (14.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,649</span> (14.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.)How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients!\n",
        "\n",
        "ANS\n",
        "\n",
        "Gradient clipping is a technique used to control the size of gradients during training, preventing exploding gradients by capping their magnitude. In Keras, gradient clipping can be implemented via the clipvalue or clipnorm arguments in optimizers.\n",
        "\n",
        "Key Gradient Clipping Methods\n",
        "\n",
        "1.Clip by Value (clipvalue):\n",
        "*   Clips gradients element-wise to lie within a specified range\n",
        "[\n",
        "−\n",
        "𝑐\n",
        ",\n",
        "𝑐\n",
        "]\n",
        "[−c,c].\n",
        "2.Clip by Norm (clipnorm):\n",
        "*   Clips gradients based on their L2 norm to not exceed a specified threshold.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EcYsM1-bsQgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.) How do you add batch normalization to a neural network model in Keras!\n",
        "\n",
        "\n",
        "ans\n",
        "\n",
        "Batch normalization (BatchNorm) is a technique used to normalize the inputs to each layer during training. It helps improve the stability of the neural network, accelerates convergence, and may act as a regularizer. In Keras, the BatchNormalization layer is used to add batch normalization to a neural network.\n",
        "\n",
        "When and Where to Use Batch Normalization\n",
        "*   Typically used before the activation function in a layer.\n",
        "*   Can be applied to:\n",
        "   *   Fully connected layers.\n",
        "   *   Convolutional layers.\n",
        "   *   Recurrent layers with caution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B3ipcT-rtGxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.) How can you create a custom loss function in Keras!\n",
        "\n",
        "ANS\n",
        "\n",
        "Creating a custom loss function in Keras allows you to define your own way of calculating the difference between predicted and actual values. A custom loss function can be useful when you need to optimize your model for a unique metric or task.\n",
        "\n",
        "In Keras, a custom loss function can be defined by writing a Python function or a subclass of tf.keras.losses.Loss. It should take the true labels and predicted outputs as arguments, and return a scalar value representing the loss.\n",
        "\n",
        "Method 1: Define a Custom Loss Function as a Python Function\n"
      ],
      "metadata": {
        "id": "rBLottcmuFdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Custom loss function example\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Example: Mean Squared Error with additional custom term\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error\n",
        "    custom_term = tf.reduce_mean(tf.abs(y_true - y_pred))  # Custom penalty term\n",
        "    return mse + 0.1 * custom_term  # Combine both terms\n"
      ],
      "metadata": {
        "id": "OtNvjp-wuWWs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Model definition\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=20, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with the custom loss\n",
        "model.compile(optimizer=Adam(), loss=custom_loss)\n",
        "\n",
        "# Train the model (assuming X_train and y_train are defined)\n",
        "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "rdf4LI62ubyx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.)How can you visualize the structure of a neural network model in Keras?\n",
        "\n",
        "ANS\n",
        "\n",
        "In Keras, you can visualize the structure of a neural network model using several methods, which provide insight into the layers, number of parameters, and architecture. Here are the most commonly used approaches:\n",
        "\n",
        "1. Using model.summary()\n",
        "\n",
        "\n",
        "The model.summary() method provides a textual representation of the model, including the layer types, output shapes, and the number of parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "lC8cBmxwvOMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=20, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "dxKKJ5DfvYwZ",
        "outputId": "232566e7-065e-4695-c846-3034bf0a474c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m1,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
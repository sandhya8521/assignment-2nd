# -*- coding: utf-8 -*-
"""Generative AI for Machine Translation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PMiWmjBmETsu8wb1O9XACuXh5cJYGfWp

1.)What is Statistical Machine Translation (SMT)?

ANS

Statistical Machine Translation (SMT) is a methodology in natural language processing (NLP) used to translate text from one language to another. It relies on statistical models to determine the likelihood of a translation being correct, rather than relying on predefined linguistic rules or patterns. SMT was one of the dominant approaches to machine translation before the advent of neural machine translation (NMT).

2.)What are the main differences between SMT and Neural Machine Translation (NMT)?

ANS

Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) are two distinct approaches to automated language translation. Here are the main differences between them:

1. Underlying Approach

*   SMT:
  *   Relies on statistical models based on bilingual corpora.
 *  Uses explicit phrase-based, word-based, or syntax-based translation models.
 *   Incorporates components such as a language model (ensuring fluency) and a translation model (mapping words/phrases between languages).

3.)Explain the concept of attention in Neural Machine Translation?

ANS

The concept of attention in Neural Machine Translation (NMT) addresses the challenge of effectively translating long sentences or sequences by allowing the model to focus on relevant parts of the input sentence when predicting each word in the output sentence. It significantly improves translation quality by considering context dynamically throughout the translation process.

Key Concepts of Attention:
*   Dynamic Focus:
*  In traditional NMT models like sequence-to-sequence (Seq2Seq), a fixed-length vector summarizes the input sentence, which can lose important details, especially for long sentences.
*  Attention mechanisms allow the model to dynamically focus on specific parts of the input sentence for each output word.

5.)What is poetry generation in generative ?

ANS

Poetry generation in generative AI refers to the use of machine learning models, especially those in the field of natural language processing (NLP), to create original poetic compositions. These models learn patterns, structures, and themes from large datasets of poems and then use this knowledge to produce coherent and stylistically appropriate verses.

6.) How does music composition with generative AI work?

ANS

Music composition with generative AI involves using machine learning algorithms to create melodies, harmonies, rhythms, or even fully orchestrated compositions. These systems analyze patterns and structures in existing musical works and use this knowledge to generate new pieces.

8.)What are multimodal generative models?

ANS

Multimodal generative models are advanced machine learning systems designed to process and generate data across multiple modalities, such as text, images, audio, and video. These models can learn representations that bridge different types of data, enabling tasks that involve integrating or transforming one modality into another.

Key Characteristics:
*   Cross-Modal Understanding:
   *   These models understand and generate outputs that link modalities, such as creating images from textual descriptions or generating captions for videos.

9.)Define Natural Language Understanding (NLU) in the context of generative ?

ANS

Natural Language Understanding (NLU) refers to a subset of Natural Language Processing (NLP) focused on enabling machines to comprehend and interpret human language in a meaningful and context-aware way. In the context of generative models, NLU serves as the foundational technology that allows machines to process input text, understand its semantics, and generate relevant, coherent, and contextually accurate outputs.

10.) What ethical considerations arise in generative AI for creative writing?

ANS

Generative AI for creative writing introduces several ethical considerations, primarily because it operates at the intersection of technology, human creativity, and cultural expression. These considerations include:

1. Authorship and Copyright
*   Attribution: Determining who owns the creative output—whether it is the AI developer, the user who prompts the AI, or neither—can be ambiguous.
*   Plagiarism: AI models can inadvertently reproduce or closely mimic copyrighted material from their training data, raising legal and ethical concerns.
*  Derivative Work: If the AI’s output resembles existing works, it may qualify as derivative, leading to potential intellectual property disputes.

11.) How can attention mechanisms improve NMT performance on longer sentences?

ANS

Attention mechanisms significantly improve Neural Machine Translation (NMT) performance on longer sentences by addressing limitations in traditional sequence-to-sequence models. Here's how attention enhances translation for such cases:

1. Focus on Relevant Input
*   The attention mechanism allows the model to dynamically focus on specific parts of the input sentence while generating each word in the output. This eliminates the need for the model to compress all input information into a single fixed-length vector, which can be challenging for long sentences.
2. Contextual Understanding
*   Attention computes a weighted sum of encoder outputs, emphasizing tokens most relevant to the current decoding step. This context-awareness ensures the model doesn't lose track of earlier words or dependencies in longer inputs.

13.) Explain how reinforcement learning differs from supervised learning in generative ?

ANS

Reinforcement Learning (RL) and Supervised Learning (SL) are distinct approaches within machine learning, each serving unique purposes in generative tasks:

Generative AI Context:

1.Supervised Learning:

*   Example: Training a generative model to produce text based on labeled datasets (e.g., GPT-2 training on structured text data).
*   Limitation: Can struggle with tasks requiring long-term coherence or creativity beyond training data.
*   Usage: Pre-training stages for language models or generating fixed patterns.
2.Reinforcement Learning:
*   Example: RLHF (Reinforcement Learning with Human Feedback) fine-tunes generative models by rewarding outputs aligning with human preferences.
*   Benefit: Encourages models to generate contextually appropriate, creative, and coherent sequences.
*  Usage: Dialog generation, game content creation, and personalized responses.

14.) What is the role of a decoder in NMT models?

ANS

In Neural Machine Translation (NMT) models, the decoder plays a crucial role in generating the output sequence, typically the translated text, based on the encoded information from the input sequence. Here's a detailed explanation of its function and significance:

16.)Describe one approach generative AI uses to avoid overfitting in creative content generation?

ANS

One approach generative AI uses to avoid overfitting in creative content generation is regularization techniques, such as dropout or weight decay. These techniques are widely employed in training neural networks to prevent them from memorizing the training data too specifically, which can lead to poor generalization to new, unseen examples.


In the context of creative AI models (such as those used in generating poetry, music, or stories), dropout randomly "drops out" or disables a fraction of the neurons during training. This helps ensure that the model doesn't become overly dependent on specific features or patterns in the data, encouraging it to learn more robust, generalized representations of creative concepts. This is especially important in creative tasks where diversity in output is key, and overfitting could result in repetitive or overly predictable content.


Weight decay penalizes large weights, encouraging the model to keep its weights small and discouraging it from fitting too closely to the noise in the training data. In generative tasks, where the AI generates novel content, this regularization helps the model to focus on essential, generalizable patterns rather than overfitting to the peculiarities of the training set.

By employing these methods, generative AI models can create diverse and novel outputs, avoiding content that is too similar to the training examples or overfit to specific details. These strategies are crucial for tasks like generating unique music, art, or writing, ensuring the results maintain creativity and variation.

18.)How does context preservation work in NMT models?

ANS

Context preservation in Neural Machine Translation (NMT) models is achieved through several mechanisms that ensure the meaning, tone, and structure of the original sentence or paragraph are preserved in the translated text. Key techniques to achieve this include:

1. Attention Mechanism:
*   The attention mechanism is crucial for preserving context in NMT. It allows the model to focus on different parts of the source sentence when generating each word of the translation. By assigning different attention scores to different words or phrases, the model can capture long-range dependencies and nuances, which are important for maintaining context, especially in longer sentences. This enables the model to avoid simply translating word-by-word and instead considers the context of the entire sentence or paragraphr-Decoder Architecture:** The encoder-decoder framework in NMT models ensures that the context is properly encoded before translation. The encoder processes the entire input sentence and converts it into a fixed-length vector (or a series of vectors in advanced models) that represents the sentence’s meaning. The decoder then generates the translation based on this representation. This structure allows for a better understanding of context, as the model has a complete understanding of the source sentence before starting the translation .
2. Conteddings:
*   Advanced NMT models, particularly those based on transformer architectures, use contextual embeddings (like BERT or GPT). These embeddings are dynamic and take into account the entire surrounding text, which helps in preserving context when translating. This dynamic representation helps the model understand the meaning of words based on their context within the sentence, avoiding errors in ambiguous words or phrases .

Q.) How does generative AI handle cultural nuances in translation?

ANS

Generative AI handles cultural nuances in translation through a combination of sophisticated models, training data, and techniques tailored to understand and incorporate cultural context. Here are some ways in which generative AI deals with these nuances:

1. Cultural Context Embeddings:

Advanced models, particularly those based on transformer architectures (like GPT, BERT, and their derivatives), can learn contextual information about different languages, cultures, and even regional dialects. These models are trained on vast datasets that include not only formal language but also regional expressions, slang, and culturally specific references. This allows the model to capture and generate more culturally appropriate translations or content, even when dealing with idiomatic expressions or local customs.


For instance, models trained on diverse and multilingual datasets can adapt translations to respect cultural sensitivities, like translating greetings or honorifics in a way that aligns with social norms in different cultures.

2. Domain-Specific Training:

Generative AI can be fine-tuned on specific domains or cultures to handle particular nuances more effectively. For example, if a model is trained on legal texts, it will be more adept at translating legal terms accurately across cultures, while a model trained on literature or film scripts will better understand figurative language, humor, and cultural references unique to a particular region or country.

Q.)Why is it difficult to fully remove bias in generative AI models?

ANS

1. Bias in Training Data:

Generative AI models, like GPT and other deep learning systems, are trained on vast datasets collected from the internet, books, articles, and more. These datasets inherently contain biases that reflect the culture, history, and societal norms of the sources. Since AI models learn patterns and associations from the data they are trained on, they can unintentionally reproduce biases related to gender, race, ethnicity, or socioeconomic status. For instance, if an AI is trained on data where certain professions or roles are predominantly associated with one gender or race, the model may reinforce these biases in its outputulty in Detecting Subtle Biases:** Bias is not always explicit. Some biases are deeply ingrained in language and society, making them difficult to detect or address. These subtle biases might not be evident in a model’s output until the model is tested in real-world scenarios, leading to problematic or harmful content. For example, AI might generate content that subtly perpetuates stereotypes or excludes certain groups, even when no overt discriminatory language is used .

2.Complthical Standards:

Bias is often context-dependent. What might be considered biased or inappropriate in one culture or community may not be seen the same way in another. AI models do not possess intrinsic ethical standards and are only as good as the data and guidelines provided during training. This makes it hard to establish a universally accepted approach to mitigating bias that works in all contexts. Different ethical frameworks and societal norms make it difficult to create a one-size-fits-all solution for AI fairness .

Q.)Implement a basic Statistical Machine Translation (SMT) model that uses word-by-word translation with a  dictionary lookup approach?

ANS

To implement a basic Statistical Machine Translation (SMT) model using a word-by-word translation approach with a dictionary lookup, we can break the problem into several steps:
"""

# Step 1: Create a translation dictionary
translation_dict = {
    'hello': 'hola',
    'world': 'mundo',
    'how': 'cómo',
    'are': 'estás',
    'you': 'tú'
}

# Step 2: Define a function to tokenize the input sentence
def tokenize(sentence):
    return sentence.lower().split()

# Step 3: Define a function to translate word-by-word using the dictionary
def translate(sentence, dictionary):
    words = tokenize(sentence)
    translated_words = []

    for word in words:
        # Look up the word in the dictionary, or use the original word if not found
        translated_word = dictionary.get(word, word)
        translated_words.append(translated_word)

    # Step 4: Join the translated words into a sentence
    return ' '.join(translated_words)

# Example usage:
source_sentence = "Hello World"
translated_sentence = translate(source_sentence, translation_dict)

print(f"Original Sentence: {source_sentence}")
print(f"Translated Sentence: {translated_sentence}")

"""Q.)Implement an Attention mechanism in a Neural Machine Translation (NMT) model using PyTorch?

ANS

Implementing an Attention Mechanism in a Neural Machine Translation (NMT) model using PyTorch requires building a mechanism that allows the model to focus on different parts of the input sequence when generating each word in the output sequence. This is achieved by assigning attention scores to the input sequence at each decoding step. Here, I will walk you through a basic implementation of an attention mechanism for NMT.


"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Define the Encoder (Simple LSTM-based)
class Encoder(nn.Module):
    def __init__(self, input_size, emb_size, hidden_size):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(input_size, emb_size)
        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True)

    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded)
        return outputs, (hidden, cell)

# Define the Attention Mechanism
class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Parameter(torch.rand(hidden_size))

    def forward(self, decoder_hidden, encoder_outputs):
        # Calculate attention scores
        seq_len = encoder_outputs.size(1)
        hidden = decoder_hidden.repeat(seq_len, 1, 1).transpose(0, 1)  # (batch, seq_len, hidden_size)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch, seq_len, hidden_size)
        attention_scores = torch.sum(energy * self.v, dim=2)  # (batch, seq_len)
        attention_weights = F.softmax(attention_scores, dim=1)  # (batch, seq_len)
        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden_size)
        return context_vector, attention_weights

# Define the Decoder with Attention
class Decoder(nn.Module):
    def __init__(self, output_size, emb_size, hidden_size, attention):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(output_size, emb_size)
        self.lstm = nn.LSTM(emb_size + hidden_size, hidden_size, batch_first=True)
        self.attention = attention
        self.fc_out = nn.Linear(hidden_size * 2, output_size)

    def forward(self, input, hidden, cell, encoder_outputs):
        # Embedding
        embedded = self.embedding(input).unsqueeze(1)

        # Apply attention to get the context vector
        context_vector, attention_weights = self.attention(hidden, encoder_outputs)

        # Concatenate the context vector with the input embedding
        rnn_input = torch.cat((embedded, context_vector), dim=2)

        # Pass through LSTM
        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))

        # Get final output from fully connected layer
        prediction = self.fc_out(torch.cat((output.squeeze(1), context_vector.squeeze(1)), dim=1))

        return prediction, hidden, cell, attention_weights

# Define the Seq2Seq model
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.size(0)
        trg_len = trg.size(1)
        batch_size = src.size(0)

        outputs = torch.zeros(batch_size, trg_len, self.decoder.fc_out.out_features).to(self.device)

        # Encoder outputs (hidden states)
        encoder_outputs, (hidden, cell) = self.encoder(src)

        # First input to the decoder is the <sos> token (start token)
        input = trg[:, 0]

        # Iterate through target sequence
        for t in range(1, trg_len):
            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[:, t] = output

            # Get the highest predicted token (teacher forcing or not)
            top1 = output.argmax(1)

            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1

        return outputs

# Hyperparameters and Initialization
input_size = 10  # example vocab size for source language
output_size = 10  # example vocab size for target language
emb_size = 32
hidden_size = 64

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

encoder = Encoder(input_size, emb_size, hidden_size).to(device)
attention = Attention(hidden_size).to(device)
decoder = Decoder(output_size, emb_size, hidden_size, attention).to(device)

model = Seq2Seq(encoder, decoder, device).to(device)

# Example input
src = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4]]).to(device)  # Batch of 2 sentences
trg = torch.tensor([[1, 5, 6, 7], [1, 5, 6, 7]]).to(device)  # Target sentences

# Example forward pass
output = model(src, trg)

print(output.shape)  # (batch_size, seq_len, output_size)

"""Q.)Use a pre-trained GPT model to perform machine translation from English to French?

ANS

To perform machine translation from English to French using a pre-trained GPT model, you can leverage a language model such as GPT-3 or GPT-4, which can be used for translation tasks with the help of the OpenAI API. Here’s how you can approach this:


"""

import openai

openai.api_key = 'your-api-key-here'

def translate_text(text, source_language='English', target_language='French'):
    prompt = f"Translate the following text from {source_language} to {target_language}: {text}"

    response = openai.Completion.create(
        engine="gpt-3.5-turbo",  # Or use "gpt-4" if you have access to GPT-4
        prompt=prompt,
        max_tokens=100,
        temperature=0.5
    )

    translation = response.choices[0].text.strip()
    return translation

text_to_translate = "Hello, how are you?"
translated_text = translate_text(text_to_translate)
print(translated_text)

"""Q.)Implement a basic reinforcement learning setup for text generation using PyTorch's reward function?

ANS

Implementing a basic reinforcement learning (RL) setup for text generation involves using an environment where the agent learns to generate text by receiving rewards based on the quality of the generated text. In this setup, the agent is typically a neural network model (like an LSTM or Transformer), and the environment is the task of generating meaningful or contextually relevant text. Below is a simplified version of such an approach using PyTorch.


"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Simple RNN Model for Text Generation (could be replaced with a Transformer model)
class TextGenerationModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(TextGenerationModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden):
        embed = self.embedding(x)
        output, hidden = self.rnn(embed, hidden)
        output = self.fc(output)
        return output, hidden

# Reward function based on similarity to reference text (for simplicity)
def reward_function(generated_text, reference_text):
    # For now, let's assume we are using a basic similarity measure, such as BLEU score
    # This can be replaced by more sophisticated metrics like ROUGE or perplexity
    return np.random.rand()  # Dummy reward

# REINFORCE Algorithm
def reinforce(model, optimizer, text_data, reference_text, max_seq_len=20):
    # Initialize hidden state
    hidden = None
    loss = 0
    total_reward = 0

    # Initialize an empty list to store log probabilities
    log_probs = []

    # Generate text token by token
    for i in range(max_seq_len):
        output, hidden = model(text_data, hidden)

        # Get the log probabilities for the current token
        probs = torch.nn.functional.softmax(output, dim=-1)
        log_prob = torch.log(probs.gather(1, text_data.unsqueeze(-1)))
        log_probs.append(log_prob)

        # Calculate the reward for the generated text so far
        reward = reward_function(text_data, reference_text)
        total_reward += reward

        # Prepare for the next token
        text_data = torch.argmax(probs, dim=-1)

    # Compute the negative log likelihood loss
    loss = -torch.stack(log_probs).sum() * total_reward

    # Update the model using the gradient
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item(), total_reward

# Example usage of the model and reinforcement learning setup
vocab_size = 5000  # Example vocabulary size
embedding_dim = 256
hidden_dim = 512

# Instantiate the model and optimizer
model = TextGenerationModel(vocab_size, embedding_dim, hidden_dim)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Example input text and reference text
input_text = torch.randint(0, vocab_size, (1, 1))  # Random token for input
reference_text = "The quick brown fox jumps over the lazy dog."  # Dummy reference

# Training loop
for epoch in range(1000):
    loss, reward = reinforce(model, optimizer, input_text, reference_text)
    print(f"Epoch {epoch}: Loss = {loss}, Reward = {reward}")

"""Q.) Create a simple multimodal generative model that generates an image caption given an image?

ANS

Creating a simple multimodal generative model that generates captions for images involves combining computer vision techniques (such as CNNs) for feature extraction from images and natural language processing (such as RNNs or Transformers) for generating captions. Here's an outline of how you can build this type of model using PyTorch.



"""

import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import numpy as np

# Step 1: Image Feature Extraction using Pre-trained CNN (ResNet)
class ImageFeatureExtractor(nn.Module):
    def __init__(self):
        super(ImageFeatureExtractor, self).__init__()
        self.resnet = models.resnet50(pretrained=True)
        self.resnet.eval()  # Set the model to evaluation mode
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the final fully connected layer

    def forward(self, x):
        with torch.no_grad():
            features = self.resnet(x)
        features = features.view(features.size(0), -1)  # Flatten the features
        return features

# Step 2: Caption Generation with LSTM
class CaptionGenerator(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super(CaptionGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim + 2048, hidden_dim)  # Concatenate image features with word embeddings
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.5)

    def forward(self, image_features, captions):
        embeddings = self.embedding(captions)
        # Concatenate image features with word embeddings at each timestep
        embeddings = torch.cat((image_features.unsqueeze(1), embeddings), dim=1)
        lstm_out, _ = self.lstm(embeddings)
        outputs = self.fc(self.dropout(lstm_out))
        return outputs

# Preprocessing for image input
def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pre-trained on ImageNet
    ])
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0)  # Add batch dimension
    return image

# Define vocabulary size, embedding and hidden dimensions
vocab_size = 5000  # Example vocabulary size (can be replaced by actual vocab size)
embedding_dim = 256
hidden_dim = 512

# Initialize models
feature_extractor = ImageFeatureExtractor()
caption_generator = CaptionGenerator(embedding_dim, hidden_dim, vocab_size)

# Example input (image path)
image_path = "sample_image.jpg"
image = preprocess_image(image_path)

# Extract image features
image_features = feature_extractor(image)

# Example input sequence (e.g., word IDs for the caption)
caption_input = torch.tensor([1, 5, 23, 56])  # Example caption (random word IDs)

# Generate caption (outputs the logits)
caption_output = caption_generator(image_features, caption_input)

# Post-process and decode the output (e.g., using a pre-defined word-to-ID map)
predicted_caption = torch.argmax(caption_output, dim=-1)
print(predicted_caption)

"""Q.)Demonstrate how to evaluate bias in generated content by analyzing GPT responses to prompts with  potentially sensitive terms?

ANS

Evaluating bias in generated content, particularly in models like GPT, involves analyzing how the model responds to prompts containing potentially sensitive or controversial terms. This process can help identify whether the model is producing biased, harmful, or unfair content based on these inputs. Here's how you can systematically evaluate this:

Q.)Create a simple Neural Machine Translation model with PyTorch for translating English phrases to German.

ANS

To create a simple Neural Machine Translation (NMT) model using PyTorch for translating English phrases into German, we will follow these steps:

1. Data Preparation:

For this simple example, we will manually create a small dataset of English-German sentence pairs. In practice, you would typically use a large parallel corpus, such as the IWSLT or Europarl datasets, but we will work with a small dataset to demonstrate the concept.

2. Model Architecture:

We will build a basic Encoder-Decoder architecture with an attention mechanism, which is common in NMT tasks.
*   Encoder: A bidirectional LSTM to process the input sequence (English sentence).

* Decoder: Another LSTM that generates the output sequence (German sentence).
"""